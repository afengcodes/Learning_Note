# 参考线

## frenet

[wiki-弗勒内-塞雷公式](https://zh.m.wikipedia.org/zh-hans/%E5%BC%97%E8%8E%B1%E7%BA%B3%E5%85%AC%E5%BC%8F)
[Frenet和笛卡尔坐标系互转](https://blog.csdn.net/qq_36458461/article/details/111935614)


Frenet坐标系的意义
（1）Frenet可以将车辆某一时刻的轨迹投影到参考线上，并分解成横向和纵向两个维度的运动，可以简化后面的轨迹规划工作。

（2）通常道路都是曲折的，这样的话用笛卡尔坐标系描述道路会非常复杂。用笛卡尔坐标系去表达道路信息，低阶多项式误差大，高阶多项式计算量大。但frenet可以忽略道路曲率。

（3）Frenet可以将车辆相对道路的运动描述的非常清晰直观。对于遵守交通规则个非常有帮助。

s代表基于参考线的纵向位移，d代表偏离参考线的横向位移


对frenet坐标系的理解：

首先frenet坐标系主要用于结构化道路，因为它需要一条参考线。
它的优点在于：
如图一所示，可以要笛卡尔坐标系不规则的道路转换为直道区域，将道路边界限制为参考线的左右边界，以s为变量，更容易之后转化为一个线性约束；同理，一些道路上障碍物也更容易转化为线性约束。而如果是用笛卡尔坐标系，则大部分情况是一个非线性约束。
更重要的是，可以把一个三维的（x,y,t）问题转化为两个二维的(s,l),(s,t)问题进行求解，能够简化问题，使轨迹规划问题的实时性更高。
缺点：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101428274.png)
见论文：李柏的《Autonomous Driving on Curvy Roads Without Reliance on Frenet Frame: A Cartesian-Based Trajectory Planning Method》

**使用**

Reference line一般是离散点，如下图所示，故投影点为近似投影。

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101510414.png)

投影点求解分3步：

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101510726.png)

## Apollo 参考线平滑
方法Fem Pos Deviation Smoother
笛卡尔坐标系下


平滑器

配置文件:

平滑器的配置文件
位置：

modules/planning/conf/planning.conf

–smoother_config_filename=/apollo/modules/planning/conf/discrete_points_smoother_config.pb.txt

内容：

max_constraint_interval : 0.25
longitudinal_boundary_bound : 2.0
max_lateral_boundary_bound : 0.5
min_lateral_boundary_bound : 0.1
curb_shift : 0.2
lateral_buffer : 0.2

discrete_points {
smoothing_method: FEM_POS_DEVIATION_SMOOTHING
fem_pos_deviation_smoothing {
weight_fem_pos_deviation: 1e10
weight_ref_deviation: 1.0
weight_path_length: 1.0
apply_curvature_constraint: false
max_iter: 500
time_limit: 0.0
verbose: false
scaled_termination: true
warm_start: true}
}
问题：

参考discretized_points_smoothing/fem_pos_deviation_smoother.h中的注释

/*

@brief:
This class solve an optimization problem:
Y
|
| P(x1, y1) P(x2, y2)
| P(x0, y0) … P(x(k-1), y(k-1))
|P(start)
|
|________________________________________________________ X
Given an initial set of points from 0 to k-1, The goal is to find a set of
points which makes the line P(start), P0, P(1) … P(k-1) “smooth”.
*/
平滑K个点即P0~PK-1。

方法：

在ReferenceLineProvider的构造函数中，有选择平滑器的相关代码：

if (smoother_config_.has_qp_spline()) {
smoother_.reset(new QpSplineReferenceLineSmoother(smoother_config_));
} else if (smoother_config_.has_spiral()) {
smoother_.reset(new SpiralReferenceLineSmoother(smoother_config_));
} else if (smoother_config_.has_discrete_points()) {
smoother_.reset(new DiscretePointsReferenceLineSmoother(smoother_config_));
} else {
ACHECK(false) << "unknown smoother config "
<< smoother_config_.DebugString();
}
Ps:apply_curvature_constraint_ =false

这分别是利用不同求解器实现了这个方法。如果考虑参考线的曲率约束，其优化问题是非线性的，可以使用ipopt非线性求解器求解（内点法），也可以使用osqp二次规划求解器来用SQP方法求解；如果不考虑曲率约束，则直接用osqp求解二次规划问题，Apollo默认使用FemPosDeviationSmoother::QpWithOsqp进行求解来减少计算量。

目标函数设计：

Apollo设计了三个代价函数，分别代表了平滑性、曲线总长度和参考点距离。大部分场景都是只考虑平滑性（平滑性权重默认设置为1e10，其他的权重设置为1）

weight_fem_pos_deviation: 1e10

weight_ref_deviation: 1.0

weight_path_length: 1.0
1
2
3
4
5
因此此处只考虑平滑性进行计算：

上述公式可以理解为：从中间那个点到第一个点的向量 和 从中间那个点到最后一个点的向量 的矢量和的模的平方。显然，如果这三个点在一条直线上，那么这个值最小；三个点组成的两个线段的夹角越小，即曲线越“弯”，这个值就越大。


---

 Apollo的的规划算法基于Frenet坐标系，因此道路中心线的平滑性控制着车辆是否左右频繁晃动，而高精地图的道路中心线往往不够平滑。

离散点平滑法，包括

1，FEM_POS_DEVIATION_SMOOTHING有限元位置差异

2，COS_THETA_SMOOTHING余弦）

3，QpSplineReferenceLineSmoother（三次样条插值法）

4，SpiralReferenceLineSmoother（螺旋曲线法）

本文只讲解FEM_POS_DEVIATION_SMOOTHING有限元位置差异的方法。

**一，为什么需要重新生成参考线**
导航路径存在的问题：1，导航路径过长；2，不平滑



导致的结果：

1，不利于主车以及障碍物寻找投影点，甚至出现多个投影点。

2，原本的导航路径不平滑，车辆无法直接进行轨迹跟踪。

解决方案：根据导航路径，生成参考线。

**二，生成参考线的具体步骤与方法**


1，找到匹配点以及投影点

2，以匹配点或者投影点为原点，往后取30个点，往前取150个点。对这180个点进行平滑处理，生成符合要求的参考线，作为该规划周期内的参考线，并以此参考线为Frenet坐标系。

**三，参考线平滑方法：QP**
方法步骤：

1，设计Cost:平滑程度，与原路径的偏离程度，路径点距离分布的均匀程度。

2，整理Cost目标函数，使其符合QP的表达形式。



3，考虑约束条件，并整理为QP的约束表达式形式。
包括：**位置约束、曲率约束（非线性 通过泰勒公式 转线性）、松弛约束**



4，运用合适的QP求解器进行求解，获取优化后的结果。

**四，设计Cost：**
**平滑程度，与原路径的偏离程度，路径点距离分布的均匀程度（长度代价）**
1，平滑程度
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101520048.png)
衡量指标：其值越小， 路径越平滑。

2，与原路径的偏离程度

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101520158.png)
如上图所示，平滑前的点记为,平滑后的点记为。

虽然路径平滑了，但与原路径距离偏差太大，不符合要求。

衡量指标：![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101520943.png)

3,路径点距离分布的均匀程度（长度代价）



衡量指标：![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101521411.png)。其值越小，分布越均匀。

**五，实例讲解**

1，以3个点为例求解目标函数

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101523987.png)


2，二次规划的一般形式
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101523442.png)


3,n个点的代价函数求解（重点）：

**平滑性代价：**
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101523525.png)


**均匀性代价（长度代价）：**

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101524628.png)


**与原路径的距离偏差代价：**

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101524579.png)


一般情况下，平滑的权重系数远远大于其他2项的权重系数

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101525761.png)

至此，参考线平滑问题转化为二次规划的标准形式，采用成熟的求解器求解即可获得,从而实现导航轨迹的平滑，获得符合要求的参考线轨迹点。

 
平滑效果：
平滑前后的坐标和曲率对比：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101521646.png)






## 粗路径优化

如规划算法中 在Frenet中， A*或dp路径规划完的粗解 优化，仍在Frenet中。

用的分段加速度 piecewisejerkproblem 中的path部分，对 P,Q重写

好像有两种形式：
1.A*后 对（s_i, l_i) 点序列直接优化
2.DP后 对（l = f(s)) 多项式参数优化 
即分段 `l_i = f_i(s) = a_i0 + a_i1 * s + ... + a_i5 * s^5`


---
如下图所示，frenet坐标系下，给定边界，使用优化的思想求解轨迹
在frenet坐标系下，规划一条在平滑（舒适）、安全（限定空间）的轨迹。
定义代价函数包括平滑性、靠近中心线、终点，在上下边界内求解使代价函数最小的目标轨迹

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101541084.png)


从以下角度出发来规划和评价path：
1 无碰撞
2 最小横向偏差 - 沿中心线行驶
3 最小横向位移 - 降低横向位移和位移频率
4 （可选）远离障碍物
从而设计cost:

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101542449.png)

subject to the safety constraint:

l(s)∈lB(s),?s∈[0,smax]
在cost中：
第一行表示平滑，l,l',l'',l'''最小
第二行表示中心线，尽量沿中心线行驶
第三行表示终点权重


具体参考下面文章，仔细学习啊！!!1


参考：

参考线平滑：
[决策规划算法二：生成参考线（FEM_POS_DEVIATION_SMOOTHING）
](https://blog.csdn.net/ChenGuiGan/article/details/124633387)

路径平滑：
[Piecewise Jerk Path Optimizer](https://www.cnblogs.com/icathianrain/p/14407626.html)

[决策规划算法四：Piecewise Jerk Path Optimizer](https://blog.csdn.net/ChenGuiGan/article/details/124638536)
[Apollo中piecewise_jerk_problem中CSC矩阵及连续性约束相关问题](https://zhuanlan.zhihu.com/p/383738586)

# 曲线

各种曲线。。

三次多项式：直线段处有曲率突变
三次样条：
    衔接点处光滑连续
    一阶、二阶导数连续可导
    自由边界 三次样条的边界二阶导连续
    单个点不影响插值曲线


# 跟驰模型

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101453089.png)

## IDM跟车模型

IDM模型是交通流仿真中一个经典的跟驰模型，包括**自由状态下的加速趋势**和**考虑与前导车碰撞的减速趋势**，其具体方程为：

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101454447.png)

IDM模型中车辆下一时刻加速度由**当前速度和最大速度的偏差**以及**当前前车距和期望跟驰距离的偏差**决定，通过将此时刻实际值与期望值做商来衡量两者之间偏差的相对值，并用**幂**计算来模拟驾驶员对这种偏差的**敏感程度**，最终实现跟驰加速度的确定，进而确定下一时刻速度和位置 ，所有车辆遵循此规则即完成交通流的跟驰建模。

**优点**：Intelligent Driver Model (IDM)模型的**参数数量少、意义明确**且与经验符合很好，并且能用统一的模型描述从自由流到完全拥堵流的不同状态。

**缺点**：**缺乏随机项**，也就是输入一定时，输出是确定的，这与现实中车辆行为的随机性有所差异。例如在交通流模拟中我们可以观察到相同参数的两辆车从路口停止线前同时起步后并行向前行驶，并在较长的时间内保持同样的行驶状态，与实际车辆驾驶行为不符。（这在我前面的文章中有提到）

时间连续的，安全距离类

---

**详细：**



**输入**：前车速度、与前车的车距、自车车速

**输出**：安全加速度


**假设条件**
- 1.自车加速度是关于**自车速度的严格递减**函数。并且如果没有其他车辆或者障碍物的阻挡，车辆倾向于加速至期望车速 v_0。
- 2.自车加速度是关于与前车或者障碍物的**距离的递增**函数。如果前车和障碍物在交互距离（视野范围）之外，则对自车不产生影响，可认为自车处于自由流状态。
- 3.自车加速度是关于**前车车速的递增**函数。结合第一点，意味着随着速度逐渐接近前车速度，自车加速度逐渐减小。同样的，前车在交互距离之外则忽略。
- 4.存在一个**最小车距** s_0 ，即使静止状态下也需要保持，如果在某个时刻两车之间小于这个最小车距，则可认为两车发生了碰撞，但是此时车辆是不可以倒车的。
- 5.平衡流状态下的车距不低于一个“安全距离”，即 `s_0 + v * T` ，其中 s_0 为最小车距，而 T 为安全车头时距。
- 6.智能控制车辆接近慢车或者障碍物或者红绿灯。
    - 1.在正常的状况下，刹车的操作应该是**轻柔**的，例如在达到一个稳定的跟驰状态前或者到达停止线前，减速度应逐渐增大至一个舒适值，然后平滑的减小到0。
    - 2.在**紧急**情况下，减速超过舒适值，直至危险解除。危险解除后如果需要应该以舒适减速度继续减速。
- 7.在不同的驾驶模式之间切换时应该是**平顺**的。例如从加速状态过渡到跟驰状态。即加速度关于时间的导数（加加速度，jerk  ）是有限的。一般认为舒适加加速度的范围为 `|jerk| <= 1.5 m / (s ^ 3)`  。也就是说加速度不可以产生突变。
- 8.模型参数应该是可解释的，并且每个模型参数只能描述驾驶行为的一个方面(这有利于模型校准)。


**数学描述**

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101321847.png)


**静止启动过程**

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101434611.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101435494.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101435124.png)


**平衡流状态**

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101436390.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101437264.png)


**高速接近时的紧急制动过程**

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101437597.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101438325.png)

参考：
[SUMO中的跟驰模型——IDM](https://zhuanlan.zhihu.com/p/428970501)
[汽车车辆运动学与机械模型](https://zhuanlan.zhihu.com/p/440798064)



# 换道模型

自由换道：
评价：
侧向加速度
轨迹曲率
起始侧向加速度

换道过程：
传统三段式： steerAngle
0 -> +max -> 0 -> -max -> 0
    扭角  靠拢  收角  调整

换道轨迹类型：
1.等速偏移
2.圆弧换道轨迹
3.梯形侧向加速度
4.正弦
5.正弦&双曲正切

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210241235295.png)

# 决策方案对比

决策算法的本质目的就是开辟凸空间，
便于后期使用二次规划QP搜索出一条符合约束条件的最优路径。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101500565.png)

最优路径的含义：使目标函数最小化的路径。

路径规划本质就是求曲线方程：l = f(s)。

开辟凸空间的方法有很多：

1，重决策的策略：分层状态机，深度学习等。根据经验规则进行决策。

2，轻决策：DP。根据代价Cost进行决策。

一，DP决策算法：
根据代价Cost进行决策
DP的过程：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101501861.png)


 

 

在frenet坐标系中，在均匀固定的的方向均匀撒点，并用5次曲线（或者其他曲线）连接各点。每条曲线的方程为：。

每条曲线都有对应的Cost,用DP算法找到Cost最小的路径，作为粗解。

并以此粗解为基础开辟凸空间。

DP结果：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210101501109.png)


二，分层状态机（或者深度学习）：根据经验规则进行决策
根据经验规则，结合主车与障碍物距离，相对速度，周围环境等场景信息，综合获得决策结果。

建立场景与决策的映射关系。

三，2种策略算法的优缺点分析
DP：

优点：

1，无需经验规则，根据代价Cost即可处理复杂的场景。

2，通过DP可获得凸空间中路径曲线的粗解，有了粗解作为基础路径，QP进行平滑时更易于求解出最优路径。

缺点：

1，场景复杂时，计算量较大

2，代价函数设计较为复杂

3，对于感知，定位精度要求相对较高

分层状态机：

优点：

1，计算量较小

2，对于感知，定位精度要求相对较低

缺点：

1，场景太多，无法完全覆盖

2，根据经验规则开辟的凸空间可能比较复杂，故通过二次规划可能无法获得最终的解。

四，应用场景
重决策：L2，L3

轻决策：L4


参考：
[决策规划算法三：DP与分层状态机2种决策算法的对比](https://blog.csdn.net/ChenGuiGan/article/details/124610182)

















---

# 待学习


## apollo算法



| 算法 | 版本 | 类型 | 说明 |
|---|---|---|---|
|RTKReplayPlanner|1.0  | RTK | 录制轨迹 |
|PublicRoadPlanner | 1.5 | PUBLIC_ROAD | EM Planner|
|NaviPlanner | 3.0 | NAVI | 实时相对地图 |
|OpenSpacePlanner | 3.5 | OPEN_SAPCE | |
百度美研&长沙智能驾驶研究室 是哪个？

其中EM 和 lattice对比

| 算法 | 流程 | 特点 | 适用场景 |
|---|---|---|---|
|EM  | 参数多DP/QP Path/Speed | 流程复杂 | 单周期，解空间受限 | 适合城市、能适应复杂场景 |
|lattice | 简单 | 简单场景解，空间较大 | 场景简单，适合高速 |




## 李柏的笛卡尔弯道规划
李柏的 笛卡尔坐标系下的规划  已有github代码，配置下运行看看。

主要工作：提出了一种迭代计算框架来累积处理复杂约束。每次迭代求解一个中间问题，其中包含线性和可处理尺度的避碰约束和软化的运动学约束。


在弯曲的道路上，由于道路趋势的曲率较大，因此需要仔细规划轨迹，确保其符合自我车的运动学，很少有能够很好地处理弯曲道路场景的方法。本文研究的是弯曲路面上的轨迹规划问题。
[Autonomous Driving on Curvy Roads Without Reliance on Frenet Frame: A Cartesian-Based Trajectory Planning Method 2021]() 原文未下载（emm ip问题？吉大可以不）
[论文阅读220403_Autonomous Driving on Curvy Roads Without Reliance on Frenet Frame: A Cartesian-Based](https://blog.csdn.net/u011841848/article/details/123931887)
[CartesianPlanner](https://github.com/libai1943/CartesianPlanner)


## 雅克比和海森矩阵

雅克比：一阶偏导 
海森：二阶偏导


[【矩阵学习】Jacobian矩阵和Hessian矩阵](https://blog.csdn.net/qq_36641919/article/details/88409718)
[机器学习中的数学——Jacobian矩阵和Hessian矩阵](https://blog.csdn.net/hy592070616/article/details/123148633)

## 凸优化
基本定义 解法    以及与mpc使用啊。。。

应用。。
罚函数：将有约束变为无约束？？    松弛

### KKT条件
库恩-塔克条件


## 数学


dy  微分 是切线的增量     
△y 差分 是曲线纵坐标的增量

微分 是 差分的线性部分。
dy = f'(x)dx
△y = y(x + △x) - y(x)
    = y'(x)△x + o(△x)
    = dy + o(△x)


## 控制

1. 不考虑横向运动模型
神经网络、模糊控制、pid
2. 车辆与期望路径横向、航向偏差
预瞄和stanley
3. 车辆动力学模型
mpc



### PID
P 响应快 调节快 （因为有延迟）会带来超调
I 消除稳态误差、但会超调
D （有延迟情况下）可以用来抑制超调


调试方法：

类型：
- 积分分离
- 抗积分饱和
- 

### 跟踪算法

MPC 需要等时间间隔
预瞄 不需要





### AEB

工况：前车静止
关键：
1.TTC值
2.执行策略

`最迟（一般0.5-1.2s） <= TTC碰撞时间 <= 驾驶员`


ttc的设定和自车速度有关。速度越高，ttc可设定更高。
v —— AEB效果、策略、制动系统性能（响应时间、最大a_b_max、保压时间）等因素 -》 TTC

在保证安全的同时，注重用户体验，产生了分级AEB
- 1.一级制动模式：直接达到很大的a_b。然后恢复
- 2.二级制动模式：
    + 先达到较大a_b, 二级制动再舒适平稳的介入
    + 先达到一较小的a_b，二级制动再舒适平稳的介入
    + 较缓的切入很大的制动，再逐渐减轻。减轻的过程中释放第二级制动。

## 预测


基础：
基于操作意图：长时间效果会好    t > t_0
运动学：短时间效果好            t < t_0
所以克综合平衡两者或其他因素。



## 机器学习

监督 无监督 强化学习

- 强化学习：ReinforcementLearning, RL
    + 不需要先验只是，并且与环境直接进行试错迭代获取反馈信息来优化策略的人工智能，并能自主学习和在线学习，但受动作空间和样本空间维数的限制。
- 深度学习：DeepLearning, DL
    + 具备较强的感知能力、能够更加适应复杂问题。但缺乏一定的决策能力。




## 车辆控制

**小知识点：**
1. 直道制动  跑偏侧滑;
    弯道制动 跑偏侧滑甩尾
2. 常见制动问题： 制动点头 制动抖动  制动噪音？ 对开路面的方向稳定性
    制动自激振动： 尤其后轴 离地（周期性短暂丧失附着） 工况：高速直线/转弯，大加速度，后轴载荷小。
    踏板振动：在附着系数变化的路面，低频振动（配有abs/esp）  工况：不同v_0下，不同a_b制动，路面：附着系数变化的平坦路面。如沥青-冰层的对开/对接路面。
3. 



航向角θ：质心速度与横轴夹角
质心侧偏角β：质心速度与车头方向夹角
车辆横摆角φ：航向角θ - 质心侧偏角β
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210222123320.png)


后轴中心点 和 质心点本质区别：
区别在于 速度角方向存在区别。


思考混合A*算法的双向？ 倒车？ 如何连接？


## 硬件

1. 终端电阻 120欧姆
若无该电阻，则信号会来回反射。 两个并联得到60欧姆。三个则40欧姆。过小相当于短路
波特率 500kbps
轮询机制
仲裁  具体见 CAN控制器




## 代码设计

1. UML图
依赖、泛化（继承）、关联、聚合（可分）、组合（不可分）、实现关系
+public -private #protected ~
抽象类用斜体



class A{
    B b;
}
class B{

}

如何初始化b呢？ 
1.构造A时对b初始化
    - 若b不被共享时
2.构造A后再将b注入
    - 若b被共享时，如文件指针

单例模式的线程安全性：
1.饿汉 -安全
2.懒汉 -不安全


## 实习 
momenta
每一个都是一个Momentum，即小m
公司：两条腿  Mpilot 数据  MSD算法
闭环自动化

三大法宝：
MF FrameWork 软件&算法
MB Box       适配硬件
MA Adaptor 方法论、工具链、部署软件

三大测：
研发迭代、采集标注、定量定性定场景


人员：
规控：Mpilot
王力leader
陈杰mentor
徐志江、张

制动过重：
几个标准：1.制动均匀 不算过重   2.自车比前车的a_b 还小，也不算（除非是前车有点刹
原因：cipv晚选、 预测不准 cutin属性获取较晚
    cipv误选（应该是误制动？）   预制动不足导致后期制动过重   
    控制：纵向超调


### SVM
模型本身  以及用法见相关Word文档。后续可整理进来。
1. 归一化 （量纲、数据范围）  影响了线性模型的权重系数
2. 特征重要性 在线性模型中即权重
3. 误差惩罚系数C  越大，即对误差要求越严格。越不能容忍分类错误。
    - 增大C，可使模型在训练集上误差越小，但易过拟合，泛化能力差。即在新的数据集上效果可能会比较差。
4. 评价模型好坏指标 PR f1_score  AUC  
5. 核函数 的核：两个样本的相似性（？ 高斯核

软硬边界：
硬：严格可分
软：容许有分类错误




svm损失因子《对异常数据 ？指分类错误数据？
线性不可分时  可考虑升维


调概率阈值：
随之阈值越来越高，认定为过重的要求就越严格，会有更多漏判。即FN。
即R 下降。因为没有制动不过重的，故不会增加FP。



**详细**





其他:

speed_planner_interface.hpp 文件有结构体定义

pnc_acc_curve.py acc分区图
PR值定义  以及计算


FTP：Fusion Tracking Prediction  融合传感预测

量产：
APA :自动寻库泊车
CP  :巡航辅助

CLA :Closed Loop Automation 闭环自动化



MPD:
MPI:

benchmark 水准、标杆
不够solid  不够石锤

兜底

机制：
非接管 手动按钮，前后各15s录包
接管 自动录包




## 工具使用
docker
s分屏
shift + 左键  选中
ctrl + insert 复制
shift + ctrl + insert 粘贴

python：
sklearn： scipy、pandas、numpy


vscode:
ctrl + T 搜文件 打开
ctrl + shift + T 恢复关闭的文件

ctrl + ` 终端

ctrl + alt + I  本代码块范围  可连续使用 逐级扩大
ctrl + shift + p 拓展括号

ctrl + shift + <- 或 -> 单元选中 可连续使用
ctrl + shift + home / end 选中到头或到尾

ctrl + backspace 删除上个单词
ctrl + home / home 到开头、结尾

shift + alt + 多选 可批量执行相同操作

ctrl + 上/下  显示换行


### 库使用
eigen3
osqp
osqp-eigen

matplotlibcpp
gnuplot



qt-ros

项目改：
1.ros版本  如noetic-》melodic 两处
2.删除除src之外的文件夹
3.qt_build删除  不能找到功能包
找不到Qt5MultiMedia。。。。   安装 qtmultimedia5-dev
配置文件路径ROS。。？

## 规控常见问题及解决

以下来自dji车载规控负责人 刘思康 公开课：**浅谈决策规划在量产自动驾驶中的挑战**

1. 自动驾驶两种模式：跨越 waymo及特种车    渐进 tesla
级别：
L4：依赖高精地图和完美感知
- 红绿灯路口场景处理较好
- 缺点
    - 成本高 覆盖率低
    - 鲜度、更新不及时
L2：不依赖高精地图、感知不确定性大
- 感知
    - 路面元素：车道线、停止线、路沿...
    - 动态物体：车辆、行人、非机动车...
    - 语义元素：红绿灯、车道方向、限速...

很常见问题，如在车道保持中：

弯道：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221843585.png)
直行：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221843012.png)
以及响应效果：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221843466.png)


2. 两种常见问题及思路
**智能性**
Interaction-aware交互:
博弈论, MCTS, POMDP, …
Spacetime时空域规划:
QP, iLQR, EM-planner, …
Learning-based数据和机器学习:
ChauffeurNet, NN-based tail-cost, IRL, …

**安全性和舒适性**
- Contingency Planning   防御性规划
- Risk-Aware Planning    预期风险规划

**目标**
Completeness完备性：扩大解空间
Optimality最优性：提升解的质量

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221845695.png)


### Contingency Planning
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221846211.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221848056.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221848016.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221849129.png)


![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221850787.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221850606.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221850881.png)


![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221850081.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221850468.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221851364.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221851390.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221852524.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221852742.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221852671.png)
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221853438.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221853892.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221853806.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221853726.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221854589.png)





### Risk-Aware Planning

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221854167.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221854754.png)

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221855955.png)


![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210221855931.png)


## 小问题

经济/博弈论  
帕累托最优 pareto optimize 
- 一方收益 另一方一定受损

纳什均衡


启发式算法： heuristic
基于直观或经验的局部优化算法
1.GA遗传算法
2.ANN人工神经网络
3.SA模拟退火
4.禁忌搜索算法

动态权重：
PSO算法中用到。但意义不同。

**PSO粒子群算法**
如找 多峰函数的最小值
与遗传算法相似，收敛于全局最优解的概率大。
1.相较于传统算法，计算速度快，全局搜索能力强
2.PSO对于群体大小不十分敏感，初始种群大小500-1000，对搜索的速度影响不大
3.粒子群算法适用于连续函数极值，对于非线性、多峰问题均有较强的全局搜索能力。

缺点：易产生早熟、收敛（尤其是处理复杂的多峰搜索问题），易陷入局部寻优。
主要原因是种群在搜索空间的多样性丢失。
即种群的多样性和收敛速度存在矛盾。

计算：
v_id = w * v_id + c1 * v1(p_id - x_id) + c2 * v2 *(pgd - xid)
     即 惯性权重   个体最优   群体最优   c1c2为学习因子
x_id = x_id + v_id


步骤：
粒子规模、位置、速度的初始化 =》 适应度函数计算  =》 个体最优值和群体最优计算 =》 位置、速度更新 =》 适应度函数更新 =》 个体、群体最优值更新  = 》 满足条件 = 输出  否则 进行位置、速度更新



### svm




#### SVM核函数

引入

类型

按核函数分：


按使用方法分：
方法一：
?	多项式思维：扩充原本的数据，制造新的多项式特征；（对每一个样本添加多项式特征）
?	步骤：
1.	PolynomialFeatures(degree = degree)：扩充原始数据，生成多项式特征；
2.	StandardScaler()：标准化处理扩充后的数据；
3.	LinearSVC(C = C)：使用 SVM 算法训练模型；
方法二：
?	使用scikit-learn 中封装好的核函数： SVC(kernel='poly', degree=degree, C=C)
?	功能：当 SVC() 的参数 kernel = ‘poly’ 时，直接使用多项式特征处理数据；
?	注：使用 SVC() 前，也需要对数据进行标准化处理


使用
多项式：
两种使用方式：多项式特征和核函数SVC
示例流程：
1）生成数据
?	datasets.make_ + 后缀：自动生成数据集；
?	如果想修改生成的数据量，可在make_moons()中填入参数；
```python 
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X, y = datasets.make_moons(noise=0.15, random_state=666)
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()
```

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231613612.png)
 
2）绘图函数

```python 
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1,1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1,1)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    
    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)
    
    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)
```

3）方法一：多项式思维


```python 
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

def PolynomialSVC(degree, C=1.0):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std)scaler', StandardScaler()),
        ('linearSVC', LinearSVC(C=C))
    ])

poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)

plot_decision_boundary(poly_svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()
```

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231613533.png)

 
?	改变参数：degree、C，模型的决策边界也跟着改变；
 
　4）方法二：使用核函数 SVC()
?	对于SVM算法，在scikit-learn的封装中，可以不使用 PolynomialFeatures的方式先将数据转化为高维的具有多项式特征的数据，在将数据提供给算法;
?	SVC() 算法：直接使用多项式特征；


```python 
from sklearn.svm import SVC
```


当算法SVC()的参数 kernel='poly'时，SVC()能直接打到一种多项式特征的效果；
使用 SVC() 前，也需要对数据进行标准化处理


```python 
def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
            ('std_scaler', StandardScaler()),
            ('kernelSVC', SVC(kernel='poly', degree=degree, C=C))
        ])

```    


```python 
poly_kernel_svc = PolynomialKernelSVC(degree=3)
poly_kernel_svc.fit(X, y)

plot_decision_boundary(poly_kernel_svc, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.show()
```

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231614117.png)
 
?	调整 PolynomialkernelSVC() 的参数：degree、C，可改决策边界；




**重点理解**
核函数**绝**就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就是避免了直接在高维空间的复杂计算

支持向量机(SVM)号称可以在保证经验风险固定较小的前提下,尽量最小化置信范围.其方法是在一个特征空间(好像要是Hilbert空间)上,找出划分两个点集的最优超平面,即使得两个点集沿该平面间隙最大.刚好落在这个间隙边上的向量叫做支持向量.这样的最大化据说就可以和置信范围最小化等价.VC维在这样的过程中是如何发生微妙变化的我还不太清楚.在两个点集不能用超平面完全划分的时候也有与此相适应的算法.推导显示求这样一个超平面,等价于求一个约束二次规划问题.
 
还有一个令人庆幸的事实是：如果只要求训练这样的超平面并对以后的数据做划分.SVM并不需要知道样例在特征空间中的向量表达或者甚至不需要知道特征空间的维数.它只需要知道任意两个样例映射到此空间后向量的内积.而这样的内积可以在样例(训练集或测试集中的)被映射成某个不可琢磨的特征空间中的向量之前被计算出来,完成这样计算的函数叫核函数.即使样例到特征空间的映射关系都不知道,只要该核函数满足伟大的Mercer定理,这样的特征空间就总能存在,SVM就能工作.







其他

简而言之，内核（kernel）是一种捷径，可以帮助我们更快地进行某些计算，否则就会涉及到更高维空间的计算。这听起来相当抽象。在这篇博文中，我将向你们展示一个只需要基本算术的简单例子。
一个简单的例子：有一个三维向量x=（x1，x2，x3）。我们将操作f（x）定义为：f（x）=
（x1x1，x1x2，x1x3.x2x1，x2x2，x2x3，x3x1，x3x2，x3x3）。换句话说，它希望x每个对相乘，并生成一个9维向量。
让我们代入数字，使它更直观！假设x =（1，2，3）；y =（4，5，6）。则：f（x）=（1，2，3，2，4，6，3，6，9）
f（y）=（16，20，24，20，25，30，24，30，36）
出于某种原因，我们实际上并不关心f（x）和f（y）。我们只想知道点积，。点积是指f（x）的第一维乘以f（y）的第一维，f（x）的第二维乘以f（y）的第二维，f（x）的第九维乘以f（y）的第九维，我们把它们加起来。所以：
= 16 + 40 + 72 + 40 + 100+ 180 + 72+ 180 + 324 = 1024很多代数！主要是因为f是从三维空间到九维空间的映射。尽管最终的答案只是一个数字，但我们必须在中间“膨胀”，在九维空间中完成所有这些乏味的计算，然后才能浓缩成单个数字。


#### svm问答

svm总结
基础
SVM就是寻找最大分类间隔的过程，使得数据点到分类超平面之间的距离最大化。
SVM是一种二分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大化是它的独特之处），通过该超平面实现对未知样本集的分类。
 
? 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机。
? 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机。
? 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

1、硬间隔：对于完全线性可分的数据集，分类全部准确，没有错误，此时的线性分类器的核心思想就是找到最大分类间隔。
2、软间隔：实际工作中的数据没有那么干净，划分数据集时容许一定量的分类错误，此时的分类间隔为软间隔。
3、对于非线性可分的数据集，引入了核函数，核函数将数据集投射到更高纬的空间，使得数据集线性可分。
4、核函数
1 多分类
SVM分类适合二分类问题，在文本分类尤其是针对二分类任务性能卓越，也可用于多分类，对多分类问题的处理有下面两种方式：

（1）一对多法：
假设要把数据集分为A、B、C、D 4个类，可以将其中一个类作为分类1，其他类作为分类2，这样我们要进行 4次SVM分类：
类别1：A 类别2：B、C、D
类别1：B 类别2：A、C、D
类别1：C 类别2：A、B、D
类别1：D 类别2：B、C、D

这种方法，针对 K 个分类，需要训练 K 个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。

（2）一对一法：
在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。
比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：
a) 分类器 1：A、B； b) 分类器 2：A、C； c) 分类器 3：B、C。

当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。

优点：如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。
缺点：分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。

2 SVM核函数意义、种类与选择
 意义：原始样本空间中可能不存在这样可以将样本正确分为两类的超平面，但是我们知道如果原始空间的维数是有限的，也就是说属性数是有限的，则一定存在一个高维特征空间能够将样本划分。SVM通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身无法线性可分的数据分开。核函数的真正意义是做到了没有真正映射到高维空间却达到了映射的作用，即减少了大量的映射计算。
 选择：
 利用专家先验知识选定核函数，例如已经知道问题是线性可分的，就可以使用线性核，不必选用非线性核。
 如果特征的数量大到和样本数量差不多，则选用线性核函数SVM或LR。
 如果特征的数量小，样本的数量正常，则选用高斯核函数SVM。
 如果特征的数量小，样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
 利用交叉验证，试用不同的核函数，误差最小的即为效果最好的核函数。
 混合核函数方法，将不同的核函数结合起来。
3 为什么要将求解SVM的原始问题转换为其对偶问题
 
? 无论原始问题是否是凸的，对偶问题都是凸优化问题；
? 对偶问题可以给出原始问题一个下界；
? 当满足一定条件（KKT条件或Slater条件）时，原始问题与对偶问题的解是完全等价的；
? 可以自然引入核函数。
 
4 SVM为什么采用间隔最大化
 当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机或神经网络等利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。
 
5 SVM对噪声敏感
 
? 少数支持向量决定了最终结果,这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单,而且具有较好的“鲁棒”性。这种“鲁棒”性主要体现在:
 增、删非支持向量样本对模型没有影响;
 支持向量样本集具有一定的鲁棒性;
 有些成功的应用中,SVM 方法对核的选取不敏感
 
? 但当噪声出现的过多，以及当噪声出现并成为支持向量时，那么噪声对模型对影响是巨大的。所以此时SVM对噪声不具备鲁棒性！以下两种情况会增大噪声成为支持向量的概率：
 噪声数量太多
 噪声以新的分布形式出现，与原先样本集的噪声分布表现的相当不同。此时噪声也有大概率落在最大分类间隔中间，从而成为支持向量，大大影响模型。
 所以我们常说的鲁棒性其实是主要是体现在对Outlier（异常点、离群点）上。

6 SVM缺失值影响
 这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，若存在缺失值它们在该特征维度很难正确的分类（例如SVM要度量距离(distance measurement)，高斯核，那么缺失值处理不当就会导致效果很差），所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。
 
7 SVM在大数据上有哪些缺陷
 SVM的空间消耗主要是在存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的内存和运算时间。如果数据量很大，SVM的训练时间就会比较长，所以SVM在大数据的使用中比较受限。
 
8 SVM之防止过拟合以及如何调节惩罚因子C
 过拟合是什么就不再解释了。SVM其实是一个自带L2正则项的分类器。SVM防止过拟合的主要技巧就在于调整软间隔松弛变量的惩罚因子C。C越大表明越不能容忍错分，当无穷大时则退化为硬间隔分类器。合适的C大小可以照顾到整体数据而不是被一个Outlier给带偏整个判决平面。至于C大小的具体调参通常可以采用交叉验证来获得。每个松弛变量对应的惩罚因子可以不一样。
 一般情况下，低偏差，高方差，即遇到过拟合时，减小C；高偏差，低方差，即遇到欠拟合时，增大C。
 
9 SVM中样本偏斜的处理方法
 样本偏斜是指数据集中正负类样本数量不均，比如正类样本有10000个，负类样本只有100个，这就可能使得超平面被“推向”负类（因为负类数量少，分布得不够广），影响结果的准确性。
 对于样本偏斜（样本不平衡）的情况，在各种机器学习方法中，我们有针对样本的通用处理办法：如上（下）采样，数据合成，加权等。
 仅在SVM中，我们可以通过为正负类样本设置不同的惩罚因子来解决样本偏斜的问题。具体做法是为负类设置大一点的惩罚因子，因为负类本来就少，不能再分错了，然后正负类的惩罚因子遵循一定的比例，比如正负类数量比为100：1，则惩罚因子的比例直接就定为1:100，具体值要通过实验确定。
 
10 SVM优缺点
 优点：
 非线性映射是SVM方法的理论基础,SVM利用内积核函数代替向高维空间的非线性映射；
 对特征空间划分的最优超平面是SVM的目标,最大化分类边际的思想是SVM方法的核心；
 支持向量是SVM的训练结果,在SVM分类决策中起决定作用的是支持向量；
 SVM 的最终决策函数只由少数的支持向量所确定,计算的复杂性取决于支持向量的数目,而不是样本空间的维数,这在某种意义上避免了“维数灾难”。
 小样本集上分类效果通常比较好。
 少数支持向量决定了最终结果,这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单,而且具有较好的“鲁棒”性。这种“鲁棒”性主要体现在:
 ①增、删非支持向量样本对模型没有影响;
 ②支持向量样本集具有一定的鲁棒性;
 ③有些成功的应用中,SVM 方法对核的选取不敏感
 SVM 是一种有坚实理论基础的新颖的小样本学习方法。它基本上不涉及概率测度及大数定律等,因此不同于现有的统计方法。从本质上看,它避开了从归纳到演绎的传统过程,实现了高效的从训练样本到预报样本的“转导推理”,大大简化了通常的分类和回归等问题。
 缺点：
 SVM算法对大规模训练样本难以实施。由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间（上面有讲）。
 用SVM解决多分类问题存在困难。传统的SVM就是解决二分类问题的，上面有介绍不少解决多分类问题的SVM技巧，不过各种方法都一定程度上的缺陷。
 对缺失值敏感，核函数的选择与调参比较复杂。
 
11 样本失衡时，如何评价分类器的性能好坏？
 答：使用ROC曲线。Roc曲线下的面积，介于0.1和1之间。AUC值是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面。Auc作为数值可以直观的评价分类器的好坏，值越大越好，随机情况大概是0.5，所以一般不错的分类器AUC至少要大于0.5。
 选择ROC和ROC下曲线面积是因为分类问题经常会碰到正负样本不均衡的问题，此时准确率和召回率不能有效评价分类器的性能，而ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。
 
12 数据不规范化对SVM的影响？
 答：大值特征会掩盖小值特征（内积计算）。高斯核会计算向量间的距离，也会产生同样的问题；多项式核会引起数值问题。影响求解的速度。数据规范化后，会丢失一些信息。预测的时候，也要进行规范化
 
13 线性核VS多项式核VS径向基核？
 答：1）训练速度：线性核只需要调节惩罚因子一个参数，所以速度快；多项式核参数多，难调;径向基核函数还需要调节γ,需要计算e的幂，所以训练速度变慢。【调参一般使用交叉验证，所以速度会慢】
 2）训练结果：线性核的得到的权重w可以反映出特征的重要性，从而进行特征选择；多项式核的结果更加直观，解释性强;径向基核得到权重是无法解释的。
 3）适应的数据：线性核：样本数量远小于特征数量(n<<m）【此时不需要映射到高维】，或者样本数量与特征数量都很大【此时主要考虑训练速度】；径向基核：样本数量远大于特征数量(n>>m)
 
14 径向基核函数中参数的物理意义
 答：如果σ选得很大的话，高次特征上的权重实际上衰减得非常快，使用泰勒展开就可以发现，当很大的时候，泰勒展开的高次项的系数会变小得很快，所以实际上相当于一个低维的子空间；
 如果σ选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题，因为此时泰勒展开式中有效的项将变得非常多，甚至无穷多，那么就相当于映射到了一个无穷维的空间，任意数据都将变得线性可分。
 
15 LR与SVM的异同
 相同
 第一，LR和SVM都是分类算法。
 第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。
 第三，LR和SVM都是监督学习算法。
 第四，LR和SVM都是判别模型。
 第五，LR和SVM都有很好的数学理论支撑。
 不同
 第一，loss function不同。
    
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231623506.png)
 
 逻辑回归损失函数
 
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231623517.png)
 
 
 支持向量机的目标函数
 
 第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。
 第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。
 第四，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。一个计算概率，一个计算距离！
 第五，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！所谓结构风险最小化，意思就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，从而达到真实误差的最小化。未达到结构风险最小化的目的，最常用的方法就是添加正则项，SVM的目标函数里居然自带正则项！！！再看一下上面提到过的SVM目标函数：
 
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231623534.png)
 
 
  
 
16 LR和SVM哪个更能对付异常点out lier？
 我们再来看看，所谓out lier，是怎么产生的，无非有两种情况，一种就是这个样本的标签y搞错了，一种就是没搞错，但这个样本是一个个例，不具备统计特性。
 不论对于哪一种情况，svm会在f将这个out lier预测的比较正确时，就停止，不会一直优化对out lier的预测，因为没有什么太大意义了。而lr则不同，它会继续要求f对这个out lier的预测进行优化，并且永不停止，显然，这样的优化很可能会削弱f的泛化性能，因为没有必要死磕out lier 。
 答案就是SVM！！！

17 特征选取
?特征提取与选择的基本任务：
是研究如何从众多特征中求出那些对分类识别最有效的特征，从而实现特征空间维数的压缩,即获取一组“少而精”且分类错误概率小的分类待征.
目的：
使在最小维数特征空间中异类模式点相距较远（类间距离较大），而同类模式点相距较近（类内距离较小）。
要求：
(1)具有很大的识别信息量。即所提供的特征应具有很好的可分性，使分类器容易判别。
(2)具有可靠性。对那些模棱两可，似是而非不易判别的特征应该去掉。
(3)具有尽可能强的独立性。重复的、相关性强的特征只选一个，因为强的相关性并没有增加更多的分类信息，不能要。
(4)数量尽可能少，同时损失的信息尽量小。
Q:为什么需要特征选择？特征选择有哪些方式？
第一个问题：
??①在现实任务中经常会遇到维数灾难问题，属性过多造成的。 ②可以降低学习任务的难度。不相关的特征就是噪声。

第二个问题：
??过滤式，包裹式，嵌入式
?? 特征选择分为两个部分，一个是子集搜索，一个是子集评价。子集搜索有前向搜索，后向搜索，双向搜索，但是都是贪心的。子集评价是通过计算属性的信息增益。即使用该特征后，降低了训练样本的不确定性。信息增益越大表明该特征对于分类作用越好。
??过滤式特征选择：RelifF思想：使类与类之间距离越大，类内差距小。对每个训练数据找到其猜中近邻(near-hit)和猜错近邻(near-miss).

??从公式可以看出，和猜错近邻的差距越大，那么该特征的重要性越高，和猜对近邻的差距越小，那么该特征的重要性越大。
注：使用RelifF需要对特征进行归一化。
包裹式特征选择：Las Vegas Wrapper(拉斯维加斯包裹)：目的选择最多的信息，最少的特征。


![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231622667.png)
 
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231622247.png)
 

Sklearn

? SVM=Support Vector Machine 是支持向量 机
? SVC=Support Vector Classification就是支持向量机用于 分类，
? SVR=Support Vector Regression.就是支持向量机用于 回归分析

SVM模型的几种
? svm.LinearSVC Linear Support Vector Classification.
? svm.LinearSVR Linear Support Vector Regression.
? svm.NuSVC Nu-Support Vector Classification.
? svm.NuSVR Nu Support Vector Regression.
? svm.OneClassSVM Unsupervised Outlier Detection.
? svm.SVC C-Support Vector Classification.
? svm.SVR Epsilon-Support Vector Regression
18 libsvm使用误区(不确定正确与否
libsvm使用误区-
（1）直接将训练集合和测试集合简单归一化到[0，1]区间，可能导致实验结果很差。
（2）如果样本的特征数非常多，那么就不必使用RBF核将样本映射到高维空间。在特征数非常多的情况下，使用线性核，结果已经非常好，并且只需要选择参数C即可。
虽然说RBF核的结果至少比线性核好，前提下搜索整个的空间。
（3）样本数<<特征数的情况：推荐使用线性核，可以达到与RBF同样的性能。
（4）样本数和特征数都非常多：推荐使用liblinear，更少的时间和内存，可比的准确率。
（5）
样本数>>特征数：如果想使用线性模型，可以使用liblinear，并且使用-s 2参数


参考

[支持向量机（SVM）常见问题](https://www.jianshu.com/p/e26d904f1996/)
[机器学习之特征选择](https://xinzhe.blog.csdn.net/article/details/84997217?spm=1001.2101.3001.6661.1&&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-84997217-blog-82984738.pc_relevant_downloadblacklistv1&&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-84997217-blog-82984738.pc_relevant_downloadblacklistv1&&utm_relevant_index=1)
[SVM详解](https://wenku.baidu.com/view/202b47f430d4b14e852458fb770bf78a65293a9b.html)
[干货 | 从超平面到SVM（一）](https://www.sohu.com/a/206572358_160850)






### 机器学习基础
机器学习
机器学习算法主要步骤有：
? 1. 数据集 选择特征并且收集并训练样本
? 一些常用的数据集，可以通过方法加载；另一种sklearn可以生成数据，可以生成你设定的数据。（设定规模，噪声等）
? 2. 数据预处理：数据预处理包括：降维、数据归一化、特征提取和特征转换（one-hot）等
? 3. 选择模型并训练 选择分类器并优化算法
? sklearn库中算法主要有四类：分类，回归，聚类，降维。其中：
? 常用的回归：线性、决策树、SVM、KNN ；集成回归：随机森林、Adaboost、GradientBoosting、Bagging、ExtraTrees
? 常用的分类：线性、决策树、SVM、KNN，朴素贝叶斯；集成分类：随机森林、Adaboost、GradientBoosting、Bagging、ExtraTrees
? 常用聚类：k均值（K-means）、层次聚类（Hierarchical clustering）、DBSCAN
? 常用降维：LinearDiscriminantAnalysis、PCA
? 有监督
? 分类/回归：
? 逻辑回归
? 支持向量机
? class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)
? 参数：
? C：正则化参数。正则化的强度与C成反比。必须严格为正。惩罚是平方的l2惩罚。(默认1.0)           C 大  ->  易过拟合
? 如果C值很大，我们的松弛变量ξi就会很小，我们允许的误差也就很小，因此分类器会尽量正确分类样本，结果我们就会得到一个小间距的超平面，这样的结果可能会导致在新样本的分类性能很差，而在训练集上的分类性能很好，也就是出现过拟合（overfitting）现象;如果C值很小，我们所允许的误差也就很大，分类器即使要错误地分类样本，它也会找寻大间距的超平面，在这种情况下，如果你的训练集是线性可分的，也有可能出现错误分类的样本。
? 因此，在SVM算法的训练上，我们可以通过减小C值来避免overfitting的发生。
? C理解为调节优化方向中两个指标（间隔大小，分类准确度）偏好的权重
? soft-margin SVM针对hard-margin SVM容易出现的过度拟合问题，适当放宽了margin的大小，容忍一些分类错误（violation），把这些样本当做噪声处理，本质上是间隔大小和噪声容忍度的一种trade-off，至于具体怎么trade-off，对哪个指标要求更高，那就体现在C这个参数上了。
? 当C趋于无穷大时，这个问题也就是不允许出现分类误差的样本存在，那这就是一个hard-margin SVM问题（过拟合）
? 当C趋于0时，我们不再关注分类是否正确，只要求间隔越大越好，那么我们将无法得到有意义的解且算法不会收敛。（欠拟合）
? kernel：核函数类型，可选‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’；
? degree：当选择核函数为poly多项式时，表示多项式的阶数
? gamma：可选‘scale’和‘auto’，表示为“ rbf”，“ poly”和“ Sigmoid”的内核系数。
? 默认是'scale',gamma取值为1 / (n_features * X.var())；当选‘auto’参数时gamma取值为1 / n_features。
? 对于高斯核函数，增大gamma值，将增大训练样本的影响范围，导致决策边界紧缩和波动；较小的gamma值得到的决策边界相对宽松。虽然较大的gamma值在训练样本中有很小的训练误差，但是很可能泛化能力较差，容易出现过拟合。 参考
? coef0：当核函数选为“ poly”和“ sigmoid”有意义。
? shrinking：是否使用缩小的启发式方法,默认是True。
? probability：是否启用概率估计,默认是False。必须在调用fit之前启用此功能，因为该方法内部使用5倍交叉验证，因而会减慢该方法的速度，并且predict_proba可能与dict不一致。
? tol：算法停止的条件，默认为0.001。cache_size：指定内核缓存的大小（以MB为单位），默认是200。
? class_weight：每个类样本的权重，可以用字典形式给出，选择'balanced',权重为n_samples / (n_classes * np.bincount(y))；默认是None，表示每个样本权重一致。
? verbose：是否使用详细输出，默认是False。
? max_iter：算法迭代的最大步数，默认-1表示无限制
? decision_function_shape：多分类的形式，1 vs 多(‘ovo’)还是1 vs 1(’ovr’)，默认’ovr’.
? break_ties：如果为true，decision_function_shape ='ovr'，并且类别数> 2，则预测将根据Decision_function的置信度值打破平局；否则，将返回绑定类中的第一类。请注意，与简单预测相比，打破平局的计算成本较高。
? random_state：随机种子，随机打乱样本。
? 3.2.2 可选标签
? support_：
? support_vectors_：支持向量
? n_support_：每个类的支持向量数量
? dual_coef_：对偶系数；
? coef_：原始问题的系数
? intercept_：决策函数中的常数
? fit_status_：如果正确拟合，则为0，否则为1（将发出警告）
? classes_：类别
? class_weight_：类别的权重
? shape_fit_：训练向量X的数组尺寸。
? 决策树
? 随机森林
? KNN 
? 回归：
? 无监督
? 聚类 clustering
? K-means
? 降维  Dimensionality Reduction
? PCA
? 模型选择   比较、验证、选择参数和模型
? 交叉验证
? 4.模型评分 评估模型性能
? 调整算法 
? 模型的保存与加载

术语
? kernel trick：
? 核方法：通过映射函数将样本的原始特征映射到一个使样本线性可分的高维空间中。
? 使用：
? 通过一个映射函数将训练集映射到高维的特征空间，并在新的空间上训练SVM，再以同样的方法应用在未知数据上。
? 映射方法面临的问题：构建新的特征空间带来非常大的计算成本。
? 使用核函数降低两点之间的内积精确计算阶段的成本：k(xi,xj)=Φ(xi)^T Φ(xj)
? 应用最广泛的核函数是径向基函数或者高斯核。
? 公式：k(xi,xj)=exp(-y||xi-xj||^2)，y=1/2σ^2
? 核：样本之间的“相似函数”
? 
? hyperplane


pip install --no-cache-dir scipy==1.6.1

#### 数据处理
数据预处理

预处理
预处理：对数据的一种简单的按特征的缩放和移动。
数据标准化 / 归一化的作用
归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。比如有一个很重要的性质：线性变换不会改变原始数据的数值排序。

? 提升模型精度：标准化 / 归一化使不同维度的特征在数值上更具比较性，提高分类器的准确性。
? 一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。
? 提升收敛速度：对于线性模型，数据归一化使梯度下降过程更加平缓，更易正确的收敛到最优解。


不同类型的预处理

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231616878.png)

这是一个有两个特征(x/y)的二分类数据集，四种预处理方法：

? StandardScaler：确保每个特征的平均值为0，方差为1。
? RobustScaler：使用中位数和四分位数（四分之一），确保每个特征的统计属性都位于同一范围。
? MinMaxScalar：移动数据，使所有特征都刚好位于0-1之间。
? Normalizer：对每个数据点进行缩放，使得特征向量的欧式长度等于1。


归一化和标准化的区别：
? 归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。
? 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。
? 它们的相同点在于都能取消由于量纲不同引起的误差；
? 都是一种线性变换，都是对向量X按照比例压缩再进行平移。
? 标准化和中心化的区别：
? 标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。



? 归一化特点：
? 对不同特征维度的伸缩变换的目的是使各个特征维度对目标函数的影响权重是一致的，即使得那些扁平分布的数据伸缩变换成类圆形。这也就改变了原始数据的一个分布。
? 好处：
提高迭代求解的收敛速度
提高迭代求解的精度
? 标准化特点：
? 对不同特征维度的伸缩变换的目的是使得不同度量之间的特征具有可比性。同时不改变原始数据的分布。
? 好处：
使得不同度量之间的特征具有可比性，对目标函数的影响体现在几何分布上，而不是数值上
不改变原始数据的分布

---

1 标准差标准化 StandardScaler
from sklearn.preprocessing import StandardScaler`
使用
使用均值与方差，对服从正态分布的数据处理，得到符合标准正态分布的数据
? 处理方法：标准化数据减去均值，然后除以标准差，经过处理后数据符合标准正态分布，即均值为0，标准差为1；
? 转化函数：x = (x-mean) / std；
? 适用性：适用于本身服从正态分布的数据；
? Outlier 的影响：基本可用于有outlier的情况，但在计算方差和均值时outliers仍然会影响计算。
原理
作用：去均值和方差归一化。且是针对每一个特征维度来做的，而不是针对样本。 
标准差标准化（standardScale）使得经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为：
`x* = (x - μ) / σ`

其中μ为所有样本数据的均值，σ为所有样本数据的标准差。





```c++  
(1)Z-score规范化（标准差标准化 / 零均值标准化）
x' = (x - μ)／σ
(2)中心化：平均值为0，对标准差无要求
x' = x - μ
```


**实现**
参数包括：with_mean, with_std, copy
? with_mean：布尔型，默认为 True，表示在缩放前将数据居中，当尝试在稀疏矩阵上时，这不起作用(并且会引发异常)，因为将它们居中需要构建一个密集矩阵，在常见的用例中，该矩阵可能太大而无法容纳在内存中；
? with_std：布尔型，默认为True，表示将数据换算成单位方差(或等效的单位标准差)；
? copy : 布尔值，默认为True，可选参数，表示拷贝一份数据以避免在原数据上进行操作，若设置为 False 执行插入行规范化并避免复制。
属性包括：mean_, scale_, var_, n_samples_seen_

? mean_：训练集中每个特征的平均值，当_mean=False时，为None；
? scale_：每个特征数据的相对缩放；
? var_：训练集中每个特征的方差，用于计算比例，当_ std =False时，为None；
? n_samples_seen_：每个特征处理的样本数。如没有丢失的样本，n_samples_seen_是一个整数，否则是一个数组，并将被重置或递增。
2 极差标准化 / 归一化 MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
使用
区间缩放，基于最大最小值，将数据转换到0,1区间上

? 处理方法：将特征缩放到给定的最小值和最大值之间，也可以将每个特征的最大绝对值转换至单位大小。这种方法是对原始数据的线性变换，将数据归一到[0,1]中间；
? 转换函数：x = (x-min) / (max-min)；
? 适用性：适用于分布范围较稳定的数据，当新数据的加入导致max/min变化，则需重新定义；
? Outlier 的影响：因为outlier会影响最大值或最小值，因此对outlier非常敏感。




```c++  
把数据变成0-1或者-1-1之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速
把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。
"""归一化"""
 x' = (x - X_min) / (X_max - X_min)
 
"""平均归一化"""
 x' = (x - μ) / (MaxValue - MinValue)
PS:上面两个公式的缺陷：当有新数据加入时，可能导致max和min的变化，需要重新定义。
"""非线性归一化"""
(1)对数函数转换：y = log10(x)
(2)反余切函数转换：y = atan(x) * 2 / π
PS: 非线性归一化经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。
#该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。
```


 
实现
参数包括：min, max, copy
? min：默认为0，指定区间的下限；
? max：默认为1，指定区间的上限；
? copy : 布尔值，默认为True，可选参数，表示拷贝一份数据以避免在原数据上进行操作，若设置为 False 执行插入行规范化并避免复制。
属性包括：min_, scale_, data_min_, data_max_
? min_：每个功能调整为最小；
? scale_：每个特征数据的相对缩放；
? data_min_：每个特征在数据中出现的最小值；
? data_max_：每个特征在数据中心出现的最大值。


3 稳健标准化 RobustScaler
from sklearn.preprocessing import RobustScaler
使用
使用具有鲁棒性的统计量缩放带有异常值（离群值）的数据

? 处理方法：该缩放器删除中位数，并根据百分位数范围（默认值为IQR：四分位间距）缩放数据；
? IQR：是第1个四分位数（25%）和第3个四分位数（75%）之间的范围；
? 适用性：适用于包含许多异常值的数据；
? Outlier 的影响：RobustScaler 利用IQR进行缩放来弱化 outlier 的影响。
实现
参数包括：with_centering, with_scaling, quantile_range, copy

? with_centering：布尔值，默认为 True，表示在缩放之前将数据居中。若使用稀疏矩阵时，这将导致转换引发异常，因为将它们居中需要建立一个密集的矩阵，在通常的使用情况下，该矩阵可能太大而无法容纳在内存中；
? with_scaling : 布尔值，默认为True，表示将数据缩放到四分位数范围；
? quantile_range : 元组，默认值为（25.0, 75.0）即 IQR，表示用于计算 scale_的分位数范围；
? copy : 布尔值，默认为True，可选参数，表示拷贝一份数据以避免在原数据上进行操作，若设置为 False 执行插入行规范化并避免复制。
属性包括：center_, scale_
? center_ ：训练集中每个属性的中位数；
? scale_ ：训练集中每个属性的四分位间距。



4 正则化
对每个样本计算其p-范数，再对每个元素除以该范数，这使得每个处理后样本的p-范数（l1-norm,l2-norm）等于1。如果后续要使用二次型等方法计算两个样本之间的相似性会有用。
`preprocessing.Normalizer(norm=’l2’, copy=True)` 


```c++  
normalizer = preprocessing.Normalizer().fit(X)
normalizer.transform(X)

array([[ 0.40824829, -0.40824829,  0.81649658],
       [ 1.        ,  0.        ,  0.        ],
       [ 0.        ,  0.70710678, -0.70710678]])

```

几个概念
1-范数：向量各分量绝对值之和
2-范数：向量长度
最大范数：向量各分量绝对值的最大值
p-范数的计算公式：||X||p=(|x1|^p+|x2|^p+…+|xn|^p)^(1/p)

总结
? 在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，StandardScaler表现更好（避免不同量纲对方差、协方差计算的影响）；
? 在不涉及距离度量、协方差、数据不符合正态分布、异常值较少的时候，可使用MinMaxScaler。（eg：图像处理中，将RGB图像转换为灰度图像后将其值限定在 [0, 255] 的范围）；
? 在带有的离群值较多的数据时，推荐使用RobustScaler。

代码
实现
手动实现 

```c++  
import numpy as np
 
x_np = np.array([[1.5, -1., 2.],
                [2., 0., 0.]])
mean = np.mean(x_np, axis=0)
std = np.std(x_np, axis=0)
print('矩阵初值为：{}'.format(x_np))
print('该矩阵的均值为：{}\n 该矩阵的标准差为：{}'.format(mean,std))
another_trans_data = x_np - mean
another_trans_data = another_trans_data / std
print('标准差标准化的矩阵为：{}'.format(another_trans_data))
---
矩阵初值为：[[ 1.5 -1.   2. ]
            [ 2.   0.   0. ]]
该矩阵的均值为：  [ 1.75 -0.5   1.  ]
该矩阵的标准差为：[0.25 0.5  1.  ]
标准差标准化的矩阵为：[[-1. -1.  1.]
```


库实现


```c++  
from sklearn.preprocessing import StandardScaler  # 标准化工具
import numpy as np
 
x_np = np.array([[1.5, -1., 2.],
                [2., 0., 0.]])
scaler = StandardScaler()
x_train = scaler.fit_transform(x_np)
print('矩阵初值为：{}'.format(x_np))
print('该矩阵的均值为：{}\n 该矩阵的标准差为：{}'.format(scaler.mean_,np.sqrt(scaler.var_)))
print('标准差标准化的矩阵为：{}'.format(x_train))
---
矩阵初值为：[[ 1.5 -1.   2. ]
            [ 2.   0.   0. ]]
该矩阵的均值为：   [ 1.75 -0.5   1.  ]
 该矩阵的标准差为：[0.25 0.5  1.  ]
标准差标准化的矩阵为：[[-1. -1.  1.]
                     [ 1.  1. -1.]]
```




问题：

逻辑回归必须要进行标准化吗？
面试的时候，无论回答必须或者不必须，都是错的！！！
是否正则化
真正的答案是，这取决于我们的逻辑回归是不是用正则。
? 如果不用正则， 那么标准化不是必须的
? 如果用正则，那么标准化是必须的
为什么
因为不用正则时，我们的损失函数只是仅仅在度量预测与真实的差距，加上正则后，我们的损失函数除了要度量上面的差距外，还要度量参数值是否足够小。而参数值的大小程度或者说大小的级别是与特征的数值范围相关的。
举例来说，我们用体重预测身高，体重用kg衡量时，训练出的模型是：
身 高 = w ? 体 重  
w就是我们训练出来的参数。
在我们的体重用吨来衡量时，w ww的值就会扩大为原来的1000倍。在上面两种情况，都用L1正则的话，显然对模型的训练影响不同。
假如不同的特征数值范围不一样，有的是0到0.1，有的是100到10000，那么，每个特征对应的参数大小级别也会不一样，在 L1 正则时，我们是简单将参数的绝对值相加，因为它们的大小级别不一样，就会导致 L1 最后只会对那些级别比较大的参数有作用，那些小的参数都被忽略了。
标准化对LR的好处
如果不用正则，那么标准化对逻辑回归有什么好处吗？
答案是有好处，进行标准化后，我们得出的参数值的大小可以反应出不同特征对样本label的贡献度，方便我们进行特征筛选。如果不做标准化，是不能这样来筛选特征的。
标准化的注意事项
最大的注意事项就是先拆分出test集，只在训练集上标准化，即均值和标准差是从训练集中计算出来的，不要在整个数据集上做标准化，因为那样会将test集的信息引入到训练集中，造成了数据信息泄露，这是一个非常容易犯的错误。



参考
重温归一化(MinMaxScaler)和标准化(StandardScaler)




#### 交叉验证
交叉验证
交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。

机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：训练集、验证集和测试集。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。

以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。

 


```python 
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

###引入数据###
iris=load_iris()
X=iris.data
y=iris.target

###训练数据###
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)
#引入交叉验证,数据分为5组进行训练
from sklearn.model_selection import cross_val_score
knn=KNeighborsClassifier(n_neighbors=5)#选择邻近的5个点
scores=cross_val_score(knn,X,y,cv=5,scoring='accuracy')#评分方式为accuracy
print(scores)#每组的评分结果
#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据
print(scores.mean())#平均评分结果
#0.973333333333
```

那么是否n_neighbor=5便是最好呢，我们来调整参数来看模型最终训练分数.

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231653629.png)


```python 
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score#引入交叉验证
import  matplotlib.pyplot as plt
###引入数据###
iris=datasets.load_iris()
X=iris.data
y=iris.target
###设置n_neighbors的值为1到30,通过绘图来看训练分数###
k_range=range(1,31)
k_score=[]
for k in k_range:
    knn=KNeighborsClassifier(n_neighbors=k)
    scores=cross_val_score(knn,X,y,cv=10,scoring='accuracy')#for classfication
    k_score.append(scores.mean())
plt.figure()
plt.plot(k_range,k_score)
plt.xlabel('Value of k for KNN')
plt.ylabel('CrossValidation accuracy')
plt.show()
#K过大会带来过拟合问题,我们可以选择12-18之间的值
```



我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择2-fold Cross Validation,Leave-One-Out Cross Validation等方法来分割数据，比较不同方法和参数得到最优结果。






#### 模型评价



模型评价


本文介绍了评价机器学习模型性能的四种方法：

（1）错误率与精度，错误率与精度是评价学习模型泛化能力的最常用的方法；
（2）从查准率和查全率的角度来评价学习模型泛化能力的优劣，并引用了P-R曲线和度量参数F1；
（3）ROC（Receiver Opreating Characteristic，受试者工作特征）曲线则评价“一般情况下”学习模型的泛化能力，并引用了度量参数AUC（Area Under Curve，曲线下的面积）；
（4）P-R曲线和ROC曲线认为学习器对不同类的分类错误产生的代价损失相同，则实际情况可能是不同类的分类错误产生的代价损失不相同，即非均衡代价，因此。从非均衡代价的角度去分析模型性能的优劣，并引用了代价曲线和期望总体代价。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231646571.png)
 
一、错误率、精度
? 错误率(Error Rate)：是分类错误的样本数占样本总数的比例。
对样例集D，分类错误率计算公式如1所示。
 
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231646501.png)
对公式（1）解释：统计分类器预测出来的结果与真实结果不相同的个数，然后除以总的样例集D的个数。
? 精度(Accuracy)：是分类正确的样本数占样本总数的比例。
对样例集D，精度计算公式如2所示。
注意：这里的分类正确的样本数指的不仅是正例分类正确的个数还有反例分类正确的个数。

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231647713.png)
对公式（2）的解释：先统计分类正确的样本数，然后除以总的样例集D的个数。
二、查准率、查全率
(1) 引入
（1）查准率、查全率出现的原因：
情景一：
错误率和精度虽然常用，但是并不能满足所有任务需求。以西瓜问题为例，假定瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，显然，错误率衡量了有多少比例的瓜被判别错误。但是若我们关心的是“挑出的西瓜中有多少比例是好瓜”，或者“所有好瓜中有多少比例被挑了出来”，那么错误率显然就不够用了，这时需要使用其他的性能度量。
情景二：
类似的需求在信息检索、Web搜索等应用中经常出现，例如在信息检索中，我们经常会关心“检索出的信息中有多少比例是用户感兴趣的”，“用户感兴趣的信息中有多少被检索出来了”。
“查准率”与“查全率”是更为合适于此类需求的性能度量。
(2) 概念
（2）什么是查准率和查全率
对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为真正例(true positive)、假正例(false positive)、真反例(true negative)、假反例(false negative)四种情形，令TP、FP、TN、FN分别表示其对应的样例数，则显然有TP+FP+TN+FN=样例总数。
分类结果的“混淆矩阵”(confusion matrix)如表1所示。
表1：分类结果混淆矩阵
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231647040.png)



```c++  
TP：真正例（true positive），即真实结果和预测结果都是正例。
FP：假正例（false positive），即真实结果是反例、预测结果是正例。
TN：真正例（true negative），即真实结果和预测结果都是反例。
FN：假反例（false negative），即真实结果是正例、预测结果是反例。
```




查准率(Precision)，又叫准确率，缩写表示用P。查准率是针对我们预测结果而言的，它表示的是预测为正的样例中有多少是真正的正样例。定义公式如3所示。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231647937.png)
注意：这里大家有一个容易混淆的误区。精度(Accuracy)和准确率(Precision)表示的是不同的概念，计算方法也不同。所以，大家在看paper的时候，要特别注意这些细节。
精确度(Accuracy)，缩写表示用A。精确度则是分类正确的样本数占样本总数的比例。Accuracy反应了分类器对整个样本的判定能力(即能将正的判定为正的，负的判定为负的)。定义公式如4所示。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231647780.png)
查全率(Recall)，又叫召回率，缩写表示用R。查全率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确。定义公式如5所示。
注意：大家可以比较一下查准率和查全率的计算公式。其实就是分母不同，查准率的分母是预测为正的样本数。查全率的分母是原样本的所有正样例数。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231648246.png)
(3) 矛盾
（3）查准率和查全率之间的矛盾
查准率和查全率是一对矛盾的度量。
一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。
思考一个问题：为什么会有这样的情况呢？
答案：我们可以这样理解，在一个分类器中，你想要更高的查准率，那么你的阈值要设置的更高，只有这样才能有较高的把握确定我们预测是正例是真正例。一旦我们把阈值设置高了，那我们预测出正例的样本数就少了，那真正例数就更少了，查不全所有的正样例。
举个例子来理解一下吧！例如，若希望将好瓜尽可能多地挑选出来，则可通过增加选瓜的数量来实现，如果将所有的西瓜都选上，那么所有的好瓜也必然都选上了，但这样查准率就会较低；若希望选出的瓜中好瓜比例尽可能高，则可只挑选最有把握的瓜，但这样就难免会漏掉不少好瓜，使得查全率较低。通常只有在一些简单任务中，才可能使查全率和查准率都很高。

三、P-R曲线、平衡点和F1度量

(1) P-R曲线
在很多情形下，我们可根据学习器的预测结果对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的是学习器认为“最不可能”是正例的样本。
按此顺序设置不同的阈值，逐个把样本作为正例进行预测，则每次可以计算出当前的查准率、查全率。以查准率为纵轴、查全率为横轴作图，就得到了查准率-查全率曲线，简称“P-R曲线”，显示该曲线的图称为“P-R图”。图1给出了一个示意图。
图1：P-R曲线与平衡点示意图
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231648060.png)


P-R图直观地显示出学习器在样本总体上的查全率、查准率。
在进行比较时，
? 若一个学习器的P-R曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者，例如图1中学习器A的性能优于学习器C；
? 如果两个学习器的P-R曲线发生了交叉，例如图1中的A和B，则难以一般性地断言两者孰优孰劣，只能在具体的查准率或查全率条件下进行比较。
? 然而，在很多情形下，人们往往仍然希望把学习器A与B比出个高低。这时，一个比较合理的判断依据是比较P-R曲线下面积的大小，它在一定程度上表征了学习器在查准率和查全率上取得相对“双高”的比例。但这个值不太容易估算，因此，人们设计了一些综合考虑查准率、查全率的性能度量，比如BEP度量、F1度量。

注： PR曲线生成：
学习模型对测试数据输出的结果是具体的数值，如逻辑斯谛克生成模型P(Y=1|X)，表示输入变量X输出为正样本的概率，曲线图如下：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231648772.png)
因此，学习模型P(Y=1|X)对测试数据集输出一系列为正样本的概率，根据概率由大到小排列，然后依次设置阈值，若大于该阈值，则为正样本；反之则为负样本。每次阈值的设置都有对应的查准率和查全率，因此以查全率为横坐标，查准率为纵坐标，就可以得到查准率-查全率曲线，检测“P-R”曲线。
P-R曲线流程图如下：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231648923.png)
(2) 平衡点(BEP)
“平衡点”(Break-Even-Point，简称BEP)就是这样一个度量，它是“查准率=查全率”时的取值，例如图1中学习器C的BEP是0.64，而基于BEP的比较，可认为学习器A优于B。
(3) F1度量
BEP曲线还是过于简化了些，更常用的是F1度量。我们先来谈谈F1度量的由来是加权调和平均，计算公式如6所示。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231649549.png)

加权调和平均与算术平均 (P+R)/2 和几何平均 sqrt(P+R) 相比，调和平均更重视较小值。
当β=1，即F1是基于查准率与查全率的调和平均定义的，公式如7所示。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231649741.png)
我们把公式7求倒数，即得F1度量公式，即公式8所示。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231649417.png)
F1越大，性能越好。
在一些应用中，对查准率和查全率的重视程度有所不同。例如在商品推荐系统中，为了尽可能少打扰用户，更希望推荐内容确实是用户感兴趣的，此时查准率更重要；而在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更重要。F1度量的一般形式是，能让我们表达出对查准率/查全率的不同偏好，它定义为公式9所示。
其中，β>0度量了查全率对查准率的相对重要性。
β=1时，退化为标准的F1；β>1时查全率有更大影响；β<1时，查准率有更大影响

四、ROC曲线与AUC
1 概念
 P-R曲线（Receiver Opreating Characteristic，受试者工作特征）是从查准率和查全率的角度去衡量学习模型的泛化性能，ROC曲线则是从更一般的情况下去衡量学习模型的泛化性能，若没有任何先验条件的限制情况下，推荐用ROC曲线去衡量模型的泛化性能。

绘制ROC曲线的思想与P-R曲线一致，对学习模型估计测试样本为正样本的概率从大到小排序，然后根据概率大小依次设置阈值，认为大于阈值的测试样本为正样本，小于阈值的测试样本为负样本，每一次阈值设置都计算“真正例率”（True Positive Rate，简称TPR）和“假正例率”（False Postive Rate，简称FPR）。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231649096.png)
 
（AB: B为预测结果，A为B的正确性。实际类型为 !(A^B) ，即 A xnor B (xnor同或 , 相同 为1，不同为0）
TPR和FPR的定义如下：

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231649089.png)
ROC曲线横坐标为假正例率，纵坐标为真正例率，曲线图如下：
 
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231650544.png)


（2）本文对ROC曲线的首末两点进行解释：

测试数据集包含N例正样本和M例负样本，若阈值设置的最大，则学习模型对所有测试样本都预测为负样本，混淆矩阵如下：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231650878.png)
TPR = TP/( TP + FN) = 0/(0+N) = 0；
FPR = FP/( TN + FP) = 0/(0+M) = 0；
因此，当阈值设置最大时，TPR与FPR均为0。
若阈值小于所有模型估计测试样本为正样本的数值时，则测试样本均为正样本，混淆矩阵如下：
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231650231.png)
TPR = TP/(TP+FN) = N/(N+0) = 1；
FPR = FP/(TN+FP) = M/(M+0) = 1；
因此，当阈值设置最小时，TPR与FPR均为1。
AUC（Area Under Curve）为ROC曲线的面积，面积可以通过梯度面积法求解。
![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231650134.png)
AUC的计算表达式理解起来有点别扭，假设正负样本数均为m例，大家回想下ROC曲线的算法思想，假正例率对应的是真实负样本中分类结果为正样本的比例，真正例率对应的是真实正样本中分类为正样本的比例。

假正例率和真正例率的增长性具有互斥性，每次都只能增加一个，且每次增加的梯度为1/m，横坐标和纵坐标共增加了m次。

理解了这个原理，相信对ROC曲线的绘制和AUC面积的计算应该有更深的认识了吧。

AUC是衡量模型泛化能力的一个重要指标，若AUC大，则分类模型优；反之，则分类模型差。想象一下，若假正例率不变的情况下，真正例率越大，对应的AUC也越大，则模型的泛化能力强，这与实际情况相符。

参考：



[机器学习模型性能评估（一）：错误率与精度](https://blog.csdn.net/algorithmPro/article/details/83870490)
[机器学习模型性能评估（二）：P-R曲线和ROC曲线](https://blog.csdn.net/algorithmPro/article/details/83870636)
[机器学习模型性能评估（三）：代价曲线](https://blog.csdn.net/algorithmPro/article/details/83870768)
[错误率、精度、查准率、查全率和F1度量】详细介绍](https://blog.csdn.net/program_developer/article/details/79937291)


其他：

[方差与偏差的区别](https://blog.csdn.net/baichoufei90/article/details/87886893?spm=1001.2101.3001.6650.1&&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-1-87886893-blog-83870907.pc_relevant_paycolumn_v3&&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-1-87886893-blog-83870907.pc_relevant_paycolumn_v3&&utm_relevant_index=2)
[模型优化的风向标：偏差与方差](https://blog.csdn.net/algorithmPro/article/details/83870907)
#### 网格搜索

网格搜索GridSearchCV
GridSearchCV 简介：
GridSearchCV，它存在的意义就是自动调参，只要把参数输进去，就能给出最优化的结果和参数。但是这个方法适合于小数据集，一旦数据的量级上去了，很难得出结果。这个时候就是需要动脑筋了。数据量比较大的时候可以使用一个快速调优的方法——坐标下降。它其实是一种贪心算法：拿当前对模型影响最大的参数调优，直到最优化；再拿下一个影响最大的参数调优，如此下去，直到所有的参数调整完毕。这个方法的缺点就是可能会调到局部最优而不是全局最优，但是省时间省力，巨大的优势面前，还是试一试吧，后续可以再拿bagging再优化。回到sklearn里面的GridSearchCV，GridSearchCV用于系统地遍历多种参数组合，通过交叉验证确定最佳效果参数。
GridSearchCV官方网址：http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
常用参数解读：
estimator：所使用的分类器，如estimator=RandomForestClassifier(min_samples_split=100,min_samples_leaf=20,max_depth=8,max_features='sqrt',random_state=10), 并且传入除需要确定最佳的参数之外的其他参数。每一个分类器都需要一个scoring参数，或者score方法。
param_grid：值为字典或者列表，即需要最优化的参数的取值，param_grid =param_test1，param_test1 = {'n_estimators':range(10,71,10)}。
scoring :准确度评价标准，默认None,这时需要使用score函数；或者如scoring='roc_auc'，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名形如：scorer(estimator, X, y)；如果是None，则使用estimator的误差估计函数。scoring参数选择如下：
参考地址：http://scikit-learn.org/stable/modules/model_evaluation.html

![](https://raw.githubusercontent.com/afengcodes/PicGoPics/main/Images/202210231640044.png)

cv :交叉验证参数，默认None，使用三折交叉验证。指定fold数量，默认为3，也可以是yield训练/测试数据的生成器。
refit :默认为True,程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与开发集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。
iid:默认True,为True时，默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。
verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，>1：对每个子模型都输出。
n_jobs: 并行数，int：个数,-1：跟CPU核数一致, 1:默认值。
pre_dispatch：指定总共分发的并行任务数。当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次
常用方法：
grid.fit()：运行网格搜索
grid_scores_：给出不同参数情况下的评价结果
best_params_：描述了已取得最佳结果的参数的组合
best_score_：成员提供优化过程期间观察到的最好的评分
使用gbm的不通用例子：



```python 
#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
import scipy as sp
import copy,os,sys,psutil
import lightgbm as lgb
from lightgbm.sklearn import LGBMRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import dump_svmlight_file
from svmutil import svm_read_problem
from sklearn import  metrics   #Additional scklearn functions
from sklearn.grid_search import GridSearchCV   #Perforing grid search
from featureProject.ly_features import make_train_set
from featureProject.my_import import split_data
# from featureProject.features import TencentReport
from featureProject.my_import import feature_importance2file
def print_best_score(gsearch,param_test):
     # 输出best score
    print("Best score: %0.3f" % gsearch.best_score_)
    print("Best parameters set:")
    # 输出最佳的分类器到底使用了怎样的参数
    best_parameters = gsearch.best_estimator_.get_params()
    for param_name in sorted(param_test.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))
def lightGBM_CV():
    print ('获取内存占用率： '+(str)(psutil.virtual_memory().percent)+'%')
    data, labels = make_train_set(24000000,25000000)
    values = data.values;
    param_test = {
        'max_depth': range(5,15,2),
        'num_leaves': range(10,40,5),
    }
    estimator = LGBMRegressor(
        num_leaves = 50, # cv调节50是最优值
        max_depth = 13,
        learning_rate =0.1, 
        n_estimators = 1000, 
        objective = 'regression', 
        min_child_weight = 1, 
        subsample = 0.8,
        colsample_bytree=0.8,
        nthread = 7,
    )
    gsearch = GridSearchCV( estimator , param_grid = param_test, scoring='roc_auc', cv=5 )
    gsearch.fit( values, labels )
    gsearch.grid_scores_, gsearch.best_params_, gsearch.best_score_
    print_best_score(gsearch,param_test)
if __name__ == '__main__':
    lightGBM_CV()
```



参考：
1. [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv)
2. [sklearn-GridSearchCV,CV调节超参使用方法](https://blog.csdn.net/u012969412/article/details/72973055?spm=1001.2101.3001.6661.1&&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-72973055-blog-83098130.pc_relevant_aa2&&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1-72973055-blog-83098130.pc_relevant_aa2&&utm_relevant_index=1)
3.	




