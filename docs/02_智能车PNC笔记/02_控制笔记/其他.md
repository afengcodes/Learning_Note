



LQR iLQR(迭代线性-二次型调节器)

LQR直接通过backward和forward过程给出最控制率和轨迹，而iLQR初始化一条轨迹，然后用LQR动态优化这条轨迹，直到找到最优解；

LQR是在环境线性化模型、cost function二次型情况下进行的，可能不能表示真实环境的情况；
于是，近似于数值优化中的思路，iLQR是将环境一阶线性化，cost function二阶泰勒近似，然后利用LQR求极值，在新极值的条件下，再次将环境一阶线性化，cost function二阶泰勒近似，求极值，指导损失函数收敛；

DDP(Differential Dynamics Programming)和ILQR的不同是将环境也进行二阶泰勒近似：
存在问题：
1、二阶近似在有些点可能不准确，求得新极值点cost function值不降反增，需要使用linear search来进行约束；
2，hessian矩阵可能不正定，需要正则化。

[如何理解强化学习中迭代线性-二次型调节器（ILQR）算法](https://blog.csdn.net/OsgoodWu/article/details/123309338)
[LQR,iLQR,DDP控制论经典算法（MBRL基础知识）](https://blog.csdn.net/weixin_40056577/article/details/104270668)
1、强化学习预备知识
（1）状态-动作价值函数Q(s, a)：
在状态s，先立即执行动作a，后面所有的状态都按照最优动作进行执行，所能获得价值之和；
（2）状态价值函数V(s)：
在状态s，从当前状态直到后面所有的状态，全部按照最优动作进行执行，所能获得的价值之和；
（3）策略函数π(s)：
已经当前的状态s，求解出最优的动作 a；

（4）Q(s, a)与V(s)之间的关系：
Q(s, a)中先执行的动作a并不一定是最优动作，而V(s)中的每一步动作a都是最优的
V(s) = max a Q(s，a)

2、LQR（Linear Quadratic Regulator）算法介绍
我们平常所说的强化学习(reinforcement learning)一般指的是Q-learning或者DDPG这类model-free的强化学习，这类方法一般是没有环境动态变化显式表达的特点，利用神经网络来近似模型；还有一种model-based的方法例如LQR或者MPC，其实这些更偏向于控制（虽然强化学习本质就是一种控制算法或者说是规划算法）

LQR算法特点：状态转移函数是线性的，而损失函数是二次型的。

算法过程：
已知初始的状态s(0)，状态转移方程为f(s, a)，以及一步状态转移导致的cost
C(s, a);
（1）backward过程：
首先假定从当前状态往下转移T（比如100）步就结束了，利用动态规划的思想，从第T步开始计算Q(s, a)，因为状态转移只到T步，和T+1步没有关系，所以可以对C(s, a)对a求导，因为C(s, a)是个二次型，想要获得的损失最小，最后一个的动作a = 0，此时可以计算出一个损失，然后从后往前推算T-1步的a，使得（T-1）的Q最大，依次类推，获取a(T), a(T-1), a(T-2), … ,a(0)；
（2）forward过程：
利用backward过程计算出来的动作序列a(T), a(T-1), a(T-2),…,a(0)，以及初始状态s(0)，能够将每一步地状态计算出来s(1), s(2),…, s(T), 以planning的角度来说，这些状态就是（p, v, a, t），优化出最好的状态之后就可以利用这些量进行执行了；

3、ILQR（Iterative LQR）算法
因为LQR算法有个很强的假设，就是状态转移方程是一次的，并且cost function是二次的，但是在实际情况中状态转移或者cost function往往是非线性的，我们类比于牛顿法和高斯牛顿法的区别，可以在初始状态的附近进行泰勒展开，f(s, a)进行一次泰勒展开， C(s, a)进行二次泰勒展开，先可以在局部收敛到一个最优值，然后在新的点再执行相同的操作，直到收敛；说白了就是有两层迭代；
 
